{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setting up the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns  # For improved visualization aesthetics\n",
    "from adjustText import adjust_text  # This library helps in adjusting text\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, explained_variance_score\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_raw = pd.read_csv(\"data/FinSalesPriceData_train.csv\")\n",
    "test_data_raw = pd.read_csv(\"data/FinSalesPriceData_test.csv\")\n",
    "competitor_data_raw = pd.read_csv(\"data/CompetitorPriceData.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>calendar_day</th>\n",
       "      <th>article_id</th>\n",
       "      <th>article_desc</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>segment</th>\n",
       "      <th>brand</th>\n",
       "      <th>brandtype</th>\n",
       "      <th>sell_price</th>\n",
       "      <th>promo_price</th>\n",
       "      <th>promo_sales</th>\n",
       "      <th>promo_units</th>\n",
       "      <th>gross_profit</th>\n",
       "      <th>scanback</th>\n",
       "      <th>sales_amount</th>\n",
       "      <th>sales_units</th>\n",
       "      <th>cnt_site_art_ranged</th>\n",
       "      <th>cnt_site_art_ranged_pstv_soh</th>\n",
       "      <th>tot_soh_ranged_sites</th>\n",
       "      <th>gst_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-02-07</td>\n",
       "      <td>196544</td>\n",
       "      <td>Skin Control Pimple Patch Micro Dart 9pk</td>\n",
       "      <td>Skin &amp; Sun Care</td>\n",
       "      <td>Skincare Face</td>\n",
       "      <td>Other</td>\n",
       "      <td>SKIN CONTROL</td>\n",
       "      <td>National Brand</td>\n",
       "      <td>13.00</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1</td>\n",
       "      <td>86.550</td>\n",
       "      <td>0.0</td>\n",
       "      <td>190.7080</td>\n",
       "      <td>15</td>\n",
       "      <td>176</td>\n",
       "      <td>171</td>\n",
       "      <td>1444.0</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-02-07</td>\n",
       "      <td>103515</td>\n",
       "      <td>Spascriptions Superfoods Masks 3x 50ml</td>\n",
       "      <td>Skin &amp; Sun Care</td>\n",
       "      <td>Skincare Face</td>\n",
       "      <td>Masks</td>\n",
       "      <td>SPASCRIPTIONS</td>\n",
       "      <td>National Brand</td>\n",
       "      <td>20.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>28.719</td>\n",
       "      <td>0.0</td>\n",
       "      <td>59.7999</td>\n",
       "      <td>3</td>\n",
       "      <td>176</td>\n",
       "      <td>175</td>\n",
       "      <td>1907.0</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-02-07</td>\n",
       "      <td>103517</td>\n",
       "      <td>Spascriptions Retinol Facial Serum</td>\n",
       "      <td>Skin &amp; Sun Care</td>\n",
       "      <td>Skincare Face</td>\n",
       "      <td>Masks</td>\n",
       "      <td>SPASCRIPTIONS</td>\n",
       "      <td>National Brand</td>\n",
       "      <td>18.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>17.158</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.8198</td>\n",
       "      <td>2</td>\n",
       "      <td>176</td>\n",
       "      <td>174</td>\n",
       "      <td>844.0</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-02-07</td>\n",
       "      <td>103518</td>\n",
       "      <td>Spascriptions Collagen Facial Serum</td>\n",
       "      <td>Skin &amp; Sun Care</td>\n",
       "      <td>Skincare Face</td>\n",
       "      <td>Masks</td>\n",
       "      <td>SPASCRIPTIONS</td>\n",
       "      <td>National Brand</td>\n",
       "      <td>18.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>42.926</td>\n",
       "      <td>0.0</td>\n",
       "      <td>89.6396</td>\n",
       "      <td>5</td>\n",
       "      <td>176</td>\n",
       "      <td>174</td>\n",
       "      <td>938.0</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-02-07</td>\n",
       "      <td>103520</td>\n",
       "      <td>7th Heaven Blackhead Stardust Face Mask</td>\n",
       "      <td>Skin &amp; Sun Care</td>\n",
       "      <td>Skincare Face</td>\n",
       "      <td>7Th Heaven</td>\n",
       "      <td>7TH HEAVEN</td>\n",
       "      <td>National Brand</td>\n",
       "      <td>6.95</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>16.919</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41.5209</td>\n",
       "      <td>6</td>\n",
       "      <td>176</td>\n",
       "      <td>173</td>\n",
       "      <td>1960.0</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  calendar_day  article_id                              article_desc  \\\n",
       "0   2022-02-07      196544  Skin Control Pimple Patch Micro Dart 9pk   \n",
       "1   2022-02-07      103515    Spascriptions Superfoods Masks 3x 50ml   \n",
       "2   2022-02-07      103517        Spascriptions Retinol Facial Serum   \n",
       "3   2022-02-07      103518       Spascriptions Collagen Facial Serum   \n",
       "4   2022-02-07      103520   7th Heaven Blackhead Stardust Face Mask   \n",
       "\n",
       "          category    subcategory     segment          brand       brandtype  \\\n",
       "0  Skin & Sun Care  Skincare Face       Other   SKIN CONTROL  National Brand   \n",
       "1  Skin & Sun Care  Skincare Face       Masks  SPASCRIPTIONS  National Brand   \n",
       "2  Skin & Sun Care  Skincare Face       Masks  SPASCRIPTIONS  National Brand   \n",
       "3  Skin & Sun Care  Skincare Face       Masks  SPASCRIPTIONS  National Brand   \n",
       "4  Skin & Sun Care  Skincare Face  7Th Heaven     7TH HEAVEN  National Brand   \n",
       "\n",
       "   sell_price  promo_price  promo_sales  promo_units  gross_profit  scanback  \\\n",
       "0       13.00         13.0         13.0            1        86.550       0.0   \n",
       "1       20.00          NaN          NaN            0        28.719       0.0   \n",
       "2       18.00          NaN          NaN            0        17.158       0.0   \n",
       "3       18.00          NaN          NaN            0        42.926       0.0   \n",
       "4        6.95          NaN          NaN            0        16.919       0.0   \n",
       "\n",
       "   sales_amount  sales_units  cnt_site_art_ranged  \\\n",
       "0      190.7080           15                  176   \n",
       "1       59.7999            3                  176   \n",
       "2       35.8198            2                  176   \n",
       "3       89.6396            5                  176   \n",
       "4       41.5209            6                  176   \n",
       "\n",
       "   cnt_site_art_ranged_pstv_soh  tot_soh_ranged_sites gst_flag  \n",
       "0                           171                1444.0        Y  \n",
       "1                           175                1907.0        Y  \n",
       "2                           174                 844.0        Y  \n",
       "3                           174                 938.0        Y  \n",
       "4                           173                1960.0        Y  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Make a copy of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a copy of the data\n",
    "data_train = train_data_raw.copy()\n",
    "data_test = test_data_raw.copy()\n",
    "data_c = competitor_data_raw.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Split the train data for train and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000013, 20)\n",
      "(250004, 20)\n",
      "(605777, 20)\n"
     ]
    }
   ],
   "source": [
    "data_train_idx, data_val_idx = train_test_split(data_train.index, test_size=0.2, random_state=3600)\n",
    "data_train = train_data_raw.loc[data_train_idx]\n",
    "data_val = train_data_raw.loc[data_val_idx]\n",
    "\n",
    "print(data_train.shape)\n",
    "print(data_val.shape)\n",
    "print(data_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Missing Value Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that process the selling price data\n",
    "def missing_sell_price(data):\n",
    "    # Calculate NA sell price using sales amount / sales units\n",
    "    data['sell_price'] = data['sell_price'].fillna(data['sales_amount']/data['sales_units'])\n",
    "    # Use previous sell price of no sales amount\n",
    "    data['sell_price'] = data['sell_price'].fillna(method='ffill')\n",
    "    return data\n",
    "\n",
    "# function that process missing promo price and promo sales\n",
    "def missing_promo(data):\n",
    "    for idx in data.index:\n",
    "        if pd.isna(data['promo_price'][idx]) and pd.notna(data['promo_sales'][idx]):\n",
    "            data['promo_sales'][idx] = np.nan\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = missing_sell_price(data_train)\n",
    "data_train = missing_promo(data_train)\n",
    "\n",
    "data_val = missing_sell_price(data_val)\n",
    "data_val = missing_promo(data_val)\n",
    "\n",
    "data_test = missing_sell_price(data_test)\n",
    "data_test = missing_promo(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000013, 20)\n",
      "(250004, 20)\n",
      "(605777, 20)\n"
     ]
    }
   ],
   "source": [
    "print(data_train.shape)\n",
    "print(data_val.shape)\n",
    "print(data_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Inconsistencies and Anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 Negative Sell Price, Promotion Price, Promotional Sales, and Scanback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(997831, 20)\n",
      "(249465, 20)\n",
      "(604344, 20)\n"
     ]
    }
   ],
   "source": [
    "# function that drop negative promotion negative, negative sell price\n",
    "def drop_neg_invalid_rows(data):\n",
    "    data = data.drop(data[data['promo_price'] < 0].index)\n",
    "    data = data.drop(data[data['promo_sales'] < 0].index)\n",
    "    data = data.drop(data[data['scanback'] < 0].index)\n",
    "    data = data.drop(data[data['sell_price'] < 0].index)\n",
    "    return data\n",
    "\n",
    "data_train = drop_neg_invalid_rows(data_train)\n",
    "data_val = drop_neg_invalid_rows(data_val)\n",
    "data_test = drop_neg_invalid_rows(data_test)\n",
    "\n",
    "print(data_train.shape)\n",
    "print(data_val.shape)\n",
    "print(data_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 Availability Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(972425, 20)\n",
      "(243254, 20)\n",
      "(596838, 20)\n"
     ]
    }
   ],
   "source": [
    "# function that drop rows with availability issues\n",
    "def rm_availability_issue(data):\n",
    "    condition = data[(data['cnt_site_art_ranged'] > 0) & (data['tot_soh_ranged_sites'] == 0)]\n",
    "    data = data.drop(condition.index)\n",
    "    return data\n",
    "\n",
    "data_train = rm_availability_issue(data_train)\n",
    "data_val = rm_availability_issue(data_val)\n",
    "data_test = rm_availability_issue(data_test)\n",
    "\n",
    "print(data_train.shape)\n",
    "print(data_val.shape)\n",
    "print(data_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3 Invalid Sales (Sales Units and Promotional Units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(919962, 20)\n",
      "(230080, 20)\n",
      "(562224, 20)\n"
     ]
    }
   ],
   "source": [
    "# function that drop rows where promo units or sales units are more than stock\n",
    "def drop_sell_more_than_stock(data):\n",
    "    data = data.drop(data[data['sales_units'] > data['tot_soh_ranged_sites']].index)\n",
    "    data = data.drop(data[data['promo_units'] > data['tot_soh_ranged_sites']].index)\n",
    "    return data\n",
    "\n",
    "data_train = drop_sell_more_than_stock(data_train)\n",
    "data_val = drop_sell_more_than_stock(data_val)\n",
    "data_test = drop_sell_more_than_stock(data_test)\n",
    "\n",
    "print(data_train.shape)\n",
    "print(data_val.shape)\n",
    "print(data_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.4 Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(919962, 20)\n",
      "(230080, 20)\n",
      "(562224, 20)\n"
     ]
    }
   ],
   "source": [
    "# function that remove duplicates\n",
    "def remove_duplicates(data):\n",
    "    duplicate = data.duplicated()\n",
    "    data = data.drop(data[duplicate].index)\n",
    "    return data\n",
    "\n",
    "data_train = remove_duplicates(data_train)\n",
    "data_val = remove_duplicates(data_val)\n",
    "data_test = remove_duplicates(data_test)\n",
    "\n",
    "print(data_train.shape)\n",
    "print(data_val.shape)\n",
    "print(data_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.5 Promo Price higher and equal to Sell Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(919146, 20)\n",
      "(229874, 20)\n",
      "(561534, 20)\n"
     ]
    }
   ],
   "source": [
    "def remove_invalid_promo(data):\n",
    "    data = data.drop(data[(data['promo_price'] >= data['sell_price'])].index)\n",
    "    return data\n",
    "\n",
    "data_train = remove_invalid_promo(data_train)\n",
    "data_val = remove_invalid_promo(data_val)\n",
    "data_test = remove_invalid_promo(data_test)\n",
    "\n",
    "print(data_train.shape)\n",
    "print(data_val.shape)\n",
    "print(data_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.5 Drop Inventory Features\n",
    "- This is because we won't know for certain what the inventory levels will be in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = data_train.drop(['cnt_site_art_ranged', 'cnt_site_art_ranged_pstv_soh', 'tot_soh_ranged_sites'], axis=1)\n",
    "data_val = data_val.drop(['cnt_site_art_ranged', 'cnt_site_art_ranged_pstv_soh', 'tot_soh_ranged_sites'], axis=1)\n",
    "data_test = data_test.drop(['cnt_site_art_ranged', 'cnt_site_art_ranged_pstv_soh', 'tot_soh_ranged_sites'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Data Types Transformation\n",
    "1. Convert `calendar_day` to datetime\n",
    "2. Create dummy variables for categorical features\n",
    "    - `article_desc` does not have a clear categorisation, so we will not create dummy for it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_to_dum(data, var_name):\n",
    "    category_dummies = pd.get_dummies(data[var_name], prefix=var_name)\n",
    "    data = pd.concat([data, category_dummies], axis=1)\n",
    "    data = data.drop(var_name, axis=1)\n",
    "    return data\n",
    "    \n",
    "def data_type_transform(data):\n",
    "    # find the categorical columns\n",
    "    cat_columns = data.select_dtypes(include=['object']).columns\n",
    "    \n",
    "    # transform calendar_day to datetime object\n",
    "    data['calendar_day'] = pd.to_datetime(data['calendar_day'])\n",
    "    \n",
    "    # transform category to dummy\n",
    "    data = cat_to_dum(data, 'category')\n",
    "    \n",
    "    # transform sub_category to dummy\n",
    "    data = cat_to_dum(data, 'subcategory')\n",
    "    \n",
    "    # transform segment to dummy\n",
    "    data = cat_to_dum(data, 'segment')\n",
    "    \n",
    "    # transform brand to dummy\n",
    "    data = cat_to_dum(data, 'brand')\n",
    "    \n",
    "    # transform brandtype to dummy\n",
    "    data = cat_to_dum(data, 'brandtype')\n",
    "    \n",
    "    # transform gst to dummy\n",
    "    data = cat_to_dum(data, 'gst_flag')\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_with_dummy = data_train.copy()\n",
    "data_train_with_dummy = data_type_transform(data_train_with_dummy)\n",
    "\n",
    "data_val_with_dummy = data_val.copy()\n",
    "data_test_with_dummy = data_test.copy()\n",
    "\n",
    "data_val_with_dummy = data_type_transform(data_val_with_dummy)\n",
    "data_test_with_dummy = data_type_transform(data_test_with_dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(919146, 413)\n",
      "(229874, 413)\n",
      "(561534, 413)\n"
     ]
    }
   ],
   "source": [
    "print(data_train_with_dummy.shape)\n",
    "print(data_val_with_dummy.shape)\n",
    "print(data_test_with_dummy.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Additional Variables\n",
    "1. Revised profits:\n",
    "    `net_proft` = `gross_profit` + `scanback`\n",
    "\n",
    "2. Cost of purchasing:\n",
    "    2.1 if GST is present: `sales_amount_exclude_gst` = `sales_amount` / 1.1\n",
    "    2.2 if GST is absent: `sales_amount_exclude_gst` = `sales_amount`\n",
    "    `cost_of_purchasing` = `sales_amount_exclude_gst` - `net_profit`\n",
    "\n",
    "3. Sales units per dollar decrease to see the price elasticity of the article:\n",
    "    `sales_units_per_dollar` = `promo_units` / (`selling_price` - `promo_price`)\n",
    "\n",
    "4. Day of Week\n",
    "    `day_of_week` = `date`.dt.dayofweek\n",
    "\n",
    "5. Whether competitor has a lower selling or shelves price\n",
    "\n",
    "6. Whether the article is on promotion (0 for no, 1 for yes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_var(data):\n",
    "    # Calculate revised net profit\n",
    "    data['net_profit'] = data['gross_profit'] + data['scanback']\n",
    "    \n",
    "    # Calculate sales amount excluding GST using vectorized operation\n",
    "    data['sales_amount_exclude_gst'] = data['sales_amount'] / (1.1 if 'gst_flag_Y' in data.columns and data['gst_flag_Y'].all() else 1)\n",
    "    \n",
    "    # Calculate cost of purchasing\n",
    "    data['cost_of_purchasing'] = data['sales_amount_exclude_gst'] - data['net_profit']\n",
    "    \n",
    "    # Calculate sales units per dollar decrease due to promotions\n",
    "    data['marginal_sales_per_dollar_decrease'] = data['promo_units'] / (data['sell_price'] - data['promo_price'])\n",
    "    \n",
    "    # Extract day of week from 'calendar_day'\n",
    "    data['day_of_week'] = data['calendar_day'].dt.dayofweek\n",
    "    ## treat day_of_week as categorical\n",
    "    data = cat_to_dum(data, 'day_of_week')\n",
    "    \n",
    "    # Identify whether the article is on promotion\n",
    "    data['on_promo'] = data['promo_price'].apply(lambda x: 1 if x > 0 else 0)\n",
    "    ## change on_promo to bool\n",
    "    data['on_promo'] = data['on_promo'].astype(bool)\n",
    "    \n",
    "    return data\n",
    "\n",
    "data_train_with_dummy = new_var(data_train_with_dummy)\n",
    "data_val_with_dummy = new_var(data_val_with_dummy)\n",
    "data_test_with_dummy = new_var(data_test_with_dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(919146, 425)\n",
      "(229874, 425)\n",
      "(561534, 425)\n"
     ]
    }
   ],
   "source": [
    "print(data_train_with_dummy.shape)\n",
    "print(data_val_with_dummy.shape)\n",
    "print(data_test_with_dummy.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Standardisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6.1 Split into normal and promotional data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(733589, 425)\n",
      "(185557, 425)\n",
      "(441888, 425)\n",
      "(119646, 425)\n",
      "(183182, 425)\n",
      "(46692, 425)\n"
     ]
    }
   ],
   "source": [
    "def normal_promo_sets(data):\n",
    "    return data[data[\"promo_price\"].isna()], data[data[\"promo_price\"].notna()]\n",
    "\n",
    "# Split data into Normal Sales and Promo Sales\n",
    "train_normal, train_promo = normal_promo_sets(data_train_with_dummy)\n",
    "test_normal, test_promo = normal_promo_sets(data_test_with_dummy)\n",
    "val_normal, val_promo = normal_promo_sets(data_val_with_dummy)\n",
    "\n",
    "# check shape\n",
    "print(train_normal.shape)\n",
    "print(train_promo.shape)\n",
    "print(test_normal.shape)\n",
    "print(test_promo.shape)\n",
    "print(val_normal.shape)\n",
    "print(val_promo.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6.2 Standardise the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['promo_price', 'promo_sales', 'marginal_sales_per_dollar_decrease'], dtype='object')\n",
      "Index([], dtype='object')\n",
      "Index([], dtype='object')\n",
      "marginal_sales_per_dollar_decrease\n",
      "2.000000      573\n",
      "4.000000      503\n",
      "1.000000      483\n",
      "0.666667      421\n",
      "1.333333      396\n",
      "             ... \n",
      "56.787330       1\n",
      "22.096317       1\n",
      "20.487805       1\n",
      "0.804598        1\n",
      "489.189189      1\n",
      "Name: count, Length: 57978, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# check numerical columns\n",
    "train_normal.select_dtypes(include=['int', 'float']).columns\n",
    "\n",
    "# colns that have na values for normal sales\n",
    "print(train_normal.columns[train_normal.isna().any()])\n",
    "\n",
    "# colns that have na values for promo sales\n",
    "print(train_promo.columns[train_promo.isna().any()])\n",
    "print(train_promo.columns[train_promo.isin([np.nan, np.inf, -np.inf]).any()])\n",
    "print(train_promo['marginal_sales_per_dollar_decrease'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(733589, 422)\n",
      "(185557, 425)\n",
      "(183182, 422)\n",
      "(46692, 425)\n",
      "(441888, 422)\n",
      "(119646, 425)\n"
     ]
    }
   ],
   "source": [
    "def prepare_for_std(data, promo=False):\n",
    "    # rm the columns that are misclassified as numerical\n",
    "    data['article_id'] = data_train_with_dummy['article_id'].astype(str)\n",
    "    \n",
    "    # consider two cases: normal sales and promo sales\n",
    "    ## for normal sales drop promo columns\n",
    "    if promo == False:\n",
    "        data = data.drop(['promo_price', 'promo_sales', 'marginal_sales_per_dollar_decrease'], axis=1)\n",
    "    return data\n",
    "\n",
    "train_normal = prepare_for_std(train_normal)\n",
    "train_promo = prepare_for_std(train_promo, promo=True)\n",
    "val_normal = prepare_for_std(val_normal)\n",
    "val_promo = prepare_for_std(val_promo, promo=True)\n",
    "test_normal = prepare_for_std(test_normal)\n",
    "test_promo = prepare_for_std(test_promo, promo=True)\n",
    "\n",
    "# check shape\n",
    "print(train_normal.shape)\n",
    "print(train_promo.shape)\n",
    "print(val_normal.shape)\n",
    "print(val_promo.shape)\n",
    "print(test_normal.shape)\n",
    "print(test_promo.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(733589, 422)\n",
      "(185557, 425)\n",
      "(183182, 422)\n",
      "(46692, 425)\n",
      "(441888, 422)\n",
      "(119646, 425)\n"
     ]
    }
   ],
   "source": [
    "train_normal_unstd = train_normal.copy()\n",
    "train_promo_unstd = train_promo.copy()\n",
    "val_normal_unstd = val_normal.copy()\n",
    "val_promo_unstd = val_promo.copy()\n",
    "test_normal_unstd = test_normal.copy()\n",
    "test_promo_unstd = test_promo.copy()\n",
    "# check shape\n",
    "print(train_normal_unstd.shape)\n",
    "print(train_promo_unstd.shape)\n",
    "print(val_normal_unstd.shape)\n",
    "print(val_promo_unstd.shape)\n",
    "print(test_normal_unstd.shape)\n",
    "print(test_promo_unstd.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(733589, 422)\n",
      "(185557, 425)\n",
      "(183182, 422)\n",
      "(46692, 425)\n",
      "(441888, 422)\n",
      "(119646, 425)\n"
     ]
    }
   ],
   "source": [
    "# standardise all numerical columns\n",
    "def standardise(data):    \n",
    "    # standardise all numerical columns\n",
    "    scaler = StandardScaler()\n",
    "    data[data.select_dtypes(include=['int', 'float']).columns] = scaler.fit_transform(data.select_dtypes(include=['int', 'float']))\n",
    "    return data\n",
    "\n",
    "train_normal = standardise(train_normal)\n",
    "train_promo = standardise(train_promo)\n",
    "val_normal = standardise(val_normal)\n",
    "val_promo = standardise(val_promo)\n",
    "test_normal = standardise(test_normal)\n",
    "test_promo = standardise(test_promo)\n",
    "\n",
    "# check shape\n",
    "print(train_normal.shape)\n",
    "print(train_promo.shape)\n",
    "print(val_normal.shape)\n",
    "print(val_promo.shape)\n",
    "print(test_normal.shape)\n",
    "print(test_promo.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 Split data into x features and y target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(733589,)\n",
      "(733589, 421)\n",
      "(185557,)\n",
      "(185557, 424)\n",
      "(441888,)\n",
      "(441888, 421)\n",
      "(119646,)\n",
      "(119646, 424)\n",
      "(183182,)\n",
      "(183182, 421)\n",
      "(46692,)\n",
      "(46692, 424)\n"
     ]
    }
   ],
   "source": [
    "def y_x_split(data):\n",
    "    return data[\"sales_units\"], data.drop(\"sales_units\", axis =1)\n",
    "\n",
    "# Split data into x and y sets\n",
    "y_train_normal, x_train_normal = y_x_split(train_normal)\n",
    "y_train_promo, x_train_promo = y_x_split(train_promo)\n",
    "\n",
    "y_test_normal, x_test_normal = y_x_split(test_normal)\n",
    "y_test_promo, x_test_promo = y_x_split(test_promo)\n",
    "\n",
    "y_val_normal, x_val_normal = y_x_split(val_normal)\n",
    "y_val_promo, x_val_promo = y_x_split(val_promo)\n",
    "\n",
    "# check shape\n",
    "print(y_train_normal.shape)\n",
    "print(x_train_normal.shape)\n",
    "print(y_train_promo.shape)\n",
    "print(x_train_promo.shape)\n",
    "\n",
    "print(y_test_normal.shape)\n",
    "print(x_test_normal.shape)\n",
    "print(y_test_promo.shape)\n",
    "print(x_test_promo.shape)\n",
    "\n",
    "print(y_val_normal.shape)\n",
    "print(x_val_normal.shape)\n",
    "print(y_val_promo.shape)\n",
    "print(x_val_promo.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>calendar_day</th>\n",
       "      <th>article_id</th>\n",
       "      <th>article_desc</th>\n",
       "      <th>sell_price</th>\n",
       "      <th>promo_price</th>\n",
       "      <th>promo_sales</th>\n",
       "      <th>promo_units</th>\n",
       "      <th>gross_profit</th>\n",
       "      <th>scanback</th>\n",
       "      <th>sales_amount</th>\n",
       "      <th>...</th>\n",
       "      <th>cost_of_purchasing</th>\n",
       "      <th>marginal_sales_per_dollar_decrease</th>\n",
       "      <th>day_of_week_0</th>\n",
       "      <th>day_of_week_1</th>\n",
       "      <th>day_of_week_2</th>\n",
       "      <th>day_of_week_3</th>\n",
       "      <th>day_of_week_4</th>\n",
       "      <th>day_of_week_5</th>\n",
       "      <th>day_of_week_6</th>\n",
       "      <th>on_promo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-05-26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Oral B VitalityFlossAction ToothBrush</td>\n",
       "      <td>1.963083</td>\n",
       "      <td>1.292222</td>\n",
       "      <td>-0.413041</td>\n",
       "      <td>-0.752682</td>\n",
       "      <td>0.400058</td>\n",
       "      <td>-0.376137</td>\n",
       "      <td>-0.024675</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011325</td>\n",
       "      <td>-0.509476</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-05-26</td>\n",
       "      <td>103520</td>\n",
       "      <td>COLGATE ADV TEETH WHITENG TPASTE 200G</td>\n",
       "      <td>-0.428994</td>\n",
       "      <td>-1.051363</td>\n",
       "      <td>-0.434208</td>\n",
       "      <td>-0.694642</td>\n",
       "      <td>0.478994</td>\n",
       "      <td>-0.383670</td>\n",
       "      <td>-0.059429</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.068998</td>\n",
       "      <td>-0.500297</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2023-05-26</td>\n",
       "      <td>103521</td>\n",
       "      <td>Colgate Adv Wht TrtrCntrl Tpaste 200g</td>\n",
       "      <td>-0.428994</td>\n",
       "      <td>-0.585949</td>\n",
       "      <td>0.514717</td>\n",
       "      <td>2.512056</td>\n",
       "      <td>-0.708254</td>\n",
       "      <td>1.206133</td>\n",
       "      <td>0.523631</td>\n",
       "      <td>...</td>\n",
       "      <td>0.391303</td>\n",
       "      <td>0.353534</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2023-05-26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>COLGATE MAX FRESH COOL MINT TPASTE 200G</td>\n",
       "      <td>-0.428994</td>\n",
       "      <td>-0.633559</td>\n",
       "      <td>-0.396393</td>\n",
       "      <td>-0.622093</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>-0.355779</td>\n",
       "      <td>-0.027621</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.057375</td>\n",
       "      <td>-0.475315</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2023-05-26</td>\n",
       "      <td>892758</td>\n",
       "      <td>Colgate Toothbrush  Zigzag Flex Med3pk</td>\n",
       "      <td>-0.548598</td>\n",
       "      <td>-0.663680</td>\n",
       "      <td>0.332281</td>\n",
       "      <td>2.395977</td>\n",
       "      <td>-0.585127</td>\n",
       "      <td>0.517941</td>\n",
       "      <td>0.281282</td>\n",
       "      <td>...</td>\n",
       "      <td>0.305521</td>\n",
       "      <td>0.572919</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 424 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   calendar_day article_id                             article_desc  \\\n",
       "1    2023-05-26        NaN    Oral B VitalityFlossAction ToothBrush   \n",
       "4    2023-05-26     103520    COLGATE ADV TEETH WHITENG TPASTE 200G   \n",
       "5    2023-05-26     103521    Colgate Adv Wht TrtrCntrl Tpaste 200g   \n",
       "7    2023-05-26        NaN  COLGATE MAX FRESH COOL MINT TPASTE 200G   \n",
       "21   2023-05-26     892758   Colgate Toothbrush  Zigzag Flex Med3pk   \n",
       "\n",
       "    sell_price  promo_price  promo_sales  promo_units  gross_profit  scanback  \\\n",
       "1     1.963083     1.292222    -0.413041    -0.752682      0.400058 -0.376137   \n",
       "4    -0.428994    -1.051363    -0.434208    -0.694642      0.478994 -0.383670   \n",
       "5    -0.428994    -0.585949     0.514717     2.512056     -0.708254  1.206133   \n",
       "7    -0.428994    -0.633559    -0.396393    -0.622093      0.538462 -0.355779   \n",
       "21   -0.548598    -0.663680     0.332281     2.395977     -0.585127  0.517941   \n",
       "\n",
       "    sales_amount  ...  cost_of_purchasing  marginal_sales_per_dollar_decrease  \\\n",
       "1      -0.024675  ...           -0.011325                           -0.509476   \n",
       "4      -0.059429  ...           -0.068998                           -0.500297   \n",
       "5       0.523631  ...            0.391303                            0.353534   \n",
       "7      -0.027621  ...           -0.057375                           -0.475315   \n",
       "21      0.281282  ...            0.305521                            0.572919   \n",
       "\n",
       "    day_of_week_0  day_of_week_1  day_of_week_2  day_of_week_3  day_of_week_4  \\\n",
       "1           False          False          False          False           True   \n",
       "4           False          False          False          False           True   \n",
       "5           False          False          False          False           True   \n",
       "7           False          False          False          False           True   \n",
       "21          False          False          False          False           True   \n",
       "\n",
       "    day_of_week_5  day_of_week_6  on_promo  \n",
       "1           False          False      True  \n",
       "4           False          False      True  \n",
       "5           False          False      True  \n",
       "7           False          False      True  \n",
       "21          False          False      True  \n",
       "\n",
       "[5 rows x 424 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_promo.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Feature Selection and Model Building\n",
    "- using pca to reduce dimention of the data (use loadings to identify the most important features)\n",
    "- then use the relatively important features to build the model, where each model will inform about the feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of principal components: 77\n",
      "Number of principal components: 50\n",
      "(733589, 172)\n",
      "(185557, 145)\n",
      "(183182, 172)\n",
      "(46692, 145)\n",
      "(441888, 172)\n",
      "(119646, 145)\n"
     ]
    }
   ],
   "source": [
    "def pca(x_data):\n",
    "    ## We set 0.95 meaning that we want to keep 95% of the variance -> i.e. pca.explained_variance_ratio_ is 0.95\n",
    "    pca = PCA(n_components=0.95, random_state=3600)\n",
    "\n",
    "    ## selecting numerical columns for PCA\n",
    "    data_num = x_data.select_dtypes(include=['int', 'float', 'bool'])\n",
    "\n",
    "    ## fit and transform the data to get the principal components\n",
    "    data_pca = pca.fit_transform(data_num)\n",
    "    \n",
    "    ## check the number of principal components that can explain 95% of the variance\n",
    "    print(f\"Number of principal components: {pca.n_components_}\")\n",
    "    \n",
    "    ## check the loadings which will be used to interpret the principal components\n",
    "    loadings = pd.DataFrame(pca.components_.T, columns=[f'PC{i+1}' for i in range(pca.components_.shape[0])], index=data_num.columns)\n",
    "\n",
    "    ## use CI to determine the threshold\n",
    "    mean_loadings = loadings.abs().mean(axis=1).sort_values(ascending=False)\n",
    "    std_loadings = loadings.abs().std(axis=1).sort_values(ascending=False)\n",
    "\n",
    "    std_error = std_loadings / np.sqrt(loadings.shape[1])\n",
    "\n",
    "    ## calculate the 95% CI\n",
    "    ci_lower = mean_loadings - 1.96 * std_error\n",
    "    ci_upper = mean_loadings + 1.96 * std_error\n",
    "\n",
    "    ## we will use a restrictive percentile (0.75) to determine the threshold\n",
    "    threshold = ci_lower.quantile(0.75)\n",
    "\n",
    "    ## select the variables that are significant\n",
    "    significant_vars = mean_loadings[mean_loadings > threshold].index\n",
    "\n",
    "    return significant_vars\n",
    "\n",
    "def dimension_red(x_data):\n",
    "    significant_features = pca(x_data)\n",
    "    x_data2 = x_data.copy()\n",
    "    x_data2 = x_data2[significant_features]\n",
    "    return x_data2\n",
    "    \n",
    "x_train_normal_pca = dimension_red(x_train_normal)\n",
    "x_train_promo_pca = dimension_red(x_train_promo)\n",
    "\n",
    "# validation and test set should contain the same columns as the training set\n",
    "x_val_normal_pca = x_val_normal[x_train_normal_pca.columns]\n",
    "x_val_promo_pca = x_val_promo[x_train_promo_pca.columns]\n",
    "\n",
    "x_test_normal_pca = x_test_normal[x_train_normal_pca.columns]\n",
    "x_test_promo_pca = x_test_promo[x_train_promo_pca.columns]\n",
    "\n",
    "print(x_train_normal_pca.shape)\n",
    "print(x_train_promo_pca.shape)\n",
    "print(x_val_normal_pca.shape)\n",
    "print(x_val_promo_pca.shape)\n",
    "print(x_test_normal_pca.shape)\n",
    "print(x_test_promo_pca.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1.1 Visualise the explained variance ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_biplot(x_data):\n",
    "    pca = PCA(n_components=2, random_state=3600)\n",
    "    data_num = x_data.select_dtypes(include=['int', 'float', 'bool'])\n",
    "    principalComponents = pca.fit_transform(data_num)\n",
    "    principalDf = pd.DataFrame(data=principalComponents, columns=['PC1', 'PC2'])\n",
    "\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Using a contrasting color 'magenta' for the scatterplot\n",
    "    sns.scatterplot(x='PC1', y='PC2', data=principalDf, alpha=0.5, color='blue')\n",
    "\n",
    "    texts = []\n",
    "    loadings = pca.components_.T * np.sqrt(pca.explained_variance_)\n",
    "\n",
    "    for i, col in enumerate(data_num.columns):\n",
    "        plt.arrow(0, 0, loadings[i, 0], loadings[i, 1], color='red', alpha=0.9, linewidth=2)\n",
    "        text = plt.text(loadings[i, 0] * 1.1, loadings[i, 1] * 1.1, col, color='black', ha='center', va='center', fontsize=12,\n",
    "                        bbox=dict(facecolor='white', edgecolor='black', boxstyle='round,pad=0.5'))\n",
    "        texts.append(text)\n",
    "\n",
    "    # Using adjustText to automatically adjust label positions\n",
    "    adjust_text(texts, arrowprops=dict(arrowstyle='->', color='black'))\n",
    "\n",
    "    plt.xlabel('Principal Component 1 (PC1)')\n",
    "    plt.ylabel('Principal Component 2 (PC2)')\n",
    "    plt.title('Enhanced PCA Biplot with Contrasting Colors')\n",
    "    plt.grid(True)\n",
    "    plt.axhline(0, color='black', linewidth=0.5)\n",
    "    plt.axvline(0, color='black', linewidth=0.5)\n",
    "    plt.show()\n",
    "    \n",
    "# pca_biplot(x_test_promo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_normal_dt = x_train_normal_pca[x_train_normal_pca.select_dtypes(include=['int', 'float', 'bool']).columns]\n",
    "x_train_promo_dt = x_train_promo_pca[x_train_promo_pca.select_dtypes(include=['int', 'float', 'bool']).columns]\n",
    "\n",
    "x_val_normal_dt = x_val_normal_pca[x_val_normal_pca.select_dtypes(include=['int', 'float', 'bool']).columns]\n",
    "x_val_promo_dt = x_val_promo_pca[x_val_promo_pca.select_dtypes(include=['int', 'float', 'bool']).columns]\n",
    "\n",
    "x_test_normal_dt = x_test_normal_pca[x_test_normal_pca.select_dtypes(include=['int', 'float', 'bool']).columns]\n",
    "x_test_promo_dt = x_test_promo_pca[x_test_promo_pca.select_dtypes(include=['int', 'float', 'bool']).columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter tuning: max_depth = 1\n",
      "Hyperparameter tuning: max_depth = 11\n",
      "Hyperparameter tuning: max_depth = 21\n",
      "Hyperparameter tuning: max_depth = 31\n",
      "Hyperparameter tuning: max_depth = 41\n",
      "Hyperparameter tuning: max_depth = 51\n",
      "Hyperparameter tuning: max_depth = 61\n",
      "Hyperparameter tuning: max_depth = 71\n",
      "Hyperparameter tuning: max_depth = 81\n",
      "Hyperparameter tuning: max_depth = 91\n",
      "Hyperparameter tuning: max_depth = 1\n",
      "Hyperparameter tuning: max_depth = 11\n",
      "Hyperparameter tuning: max_depth = 21\n",
      "Hyperparameter tuning: max_depth = 31\n",
      "Hyperparameter tuning: max_depth = 41\n",
      "Hyperparameter tuning: max_depth = 51\n",
      "Hyperparameter tuning: max_depth = 61\n",
      "Hyperparameter tuning: max_depth = 71\n",
      "Hyperparameter tuning: max_depth = 81\n",
      "Hyperparameter tuning: max_depth = 91\n"
     ]
    }
   ],
   "source": [
    "# evaluate the rf model\n",
    "def evaluate_dt(X_train, y_train, X_val, y_val, X_test, y_test):\n",
    "\n",
    "    X_train_dt = X_train\n",
    "    X_val_dt = X_val\n",
    "    X_test_dt = X_test\n",
    "    \n",
    "    ## hyperparameter tuning\n",
    "    best_rmse = float('inf')\n",
    "    best_max_depth = 0\n",
    "    for i in np.arange(1, 100, 10):\n",
    "        print(f\"Hyperparameter tuning: max_depth = {i}\")\n",
    "        dt = DecisionTreeRegressor(max_depth=i, random_state=3600)\n",
    "        dt.fit(X_train_dt, y_train)\n",
    "        y_val_pred = dt.predict(X_val_dt)\n",
    "        val_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
    "        if val_rmse < best_rmse:\n",
    "            best_rmse = val_rmse\n",
    "            best_max_depth = i\n",
    "    \n",
    "    ## best model\n",
    "    best_dt = DecisionTreeRegressor(max_depth=best_max_depth, random_state=3600)\n",
    "    best_dt.fit(X_train_dt, y_train)\n",
    "    \n",
    "    ## evaluate on test set\n",
    "    y_test_pred = best_dt.predict(X_test_dt)\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "    \n",
    "    return best_dt, best_rmse, test_rmse\n",
    "\n",
    "dt_normal, val_rmse_normal_dt, test_rmse_normal_dt = evaluate_dt(x_train_normal_dt, y_train_normal, x_val_normal_dt, y_val_normal, x_test_normal_dt, y_test_normal)\n",
    "dt_promo, val_rmse_promo_dt, test_rmse_promo_dt = evaluate_dt(x_train_promo_dt, y_train_promo, x_val_promo_dt, y_val_promo, x_test_promo_dt, y_test_promo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Best Model Parameters</th>\n",
       "      <th>Top 10 Important Features</th>\n",
       "      <th>Validation RMSE</th>\n",
       "      <th>Test RMSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Decision Tree for Normal Sales</td>\n",
       "      <td>71</td>\n",
       "      <td>[sales_amount, sell_price, cost_of_purchasing, sales_amount_exclude_gst, gross_profit, net_profit, brandtype_Private Label, category_Skin &amp; Sun Care, subcategory_Soaps &amp; Wash, segment_Kitchen]</td>\n",
       "      <td>0.040936</td>\n",
       "      <td>0.092437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Decision Tree for Promo Sales</td>\n",
       "      <td>31</td>\n",
       "      <td>[promo_units, cost_of_purchasing, brandtype_National Brand, sales_amount_exclude_gst, sell_price, sales_amount, marginal_sales_per_dollar_decrease, gross_profit, segment_Huggies Essentials, brand_REXONA]</td>\n",
       "      <td>0.115428</td>\n",
       "      <td>0.114445</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Model  Best Model Parameters  \\\n",
       "0  Decision Tree for Normal Sales                     71   \n",
       "1   Decision Tree for Promo Sales                     31   \n",
       "\n",
       "                                                                                                                                                                                     Top 10 Important Features  \\\n",
       "0             [sales_amount, sell_price, cost_of_purchasing, sales_amount_exclude_gst, gross_profit, net_profit, brandtype_Private Label, category_Skin & Sun Care, subcategory_Soaps & Wash, segment_Kitchen]   \n",
       "1  [promo_units, cost_of_purchasing, brandtype_National Brand, sales_amount_exclude_gst, sell_price, sales_amount, marginal_sales_per_dollar_decrease, gross_profit, segment_Huggies Essentials, brand_REXONA]   \n",
       "\n",
       "   Validation RMSE  Test RMSE  \n",
       "0         0.040936   0.092437  \n",
       "1         0.115428   0.114445  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# important features\n",
    "important_features_normal = pd.DataFrame({'Feature': x_train_normal_dt.columns, 'Importance': dt_normal.feature_importances_})\n",
    "important_features_normal = important_features_normal.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "important_features_promo = pd.DataFrame({'Feature': x_train_promo_dt.columns, 'Importance': dt_promo.feature_importances_})\n",
    "important_features_promo = important_features_promo.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# format the output in a table\n",
    "output = pd.DataFrame({'Model': ['Decision Tree for Normal Sales', 'Decision Tree for Promo Sales'],\n",
    "                       'Best Model Parameters': [dt_normal.max_depth, dt_promo.max_depth],\n",
    "                       'Top 10 Important Features': [important_features_normal['Feature'].head(10).values, important_features_promo['Feature'].head(10).values],\n",
    "                       'Validation RMSE': [val_rmse_normal_dt, val_rmse_promo_dt],\n",
    "                       'Test RMSE': [test_rmse_normal_dt, test_rmse_promo_dt]})\n",
    "                      \n",
    "pd.set_option('display.max_colwidth', None)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_normal_rf = x_train_normal_pca[x_train_normal_pca.select_dtypes(include=['int', 'float', 'bool']).columns]\n",
    "x_train_promo_rf = x_train_promo_pca[x_train_promo_pca.select_dtypes(include=['int', 'float', 'bool']).columns]\n",
    "\n",
    "x_test_normal_rf = x_test_normal_pca[x_test_normal_pca.select_dtypes(include=['int', 'float', 'bool']).columns]\n",
    "x_test_promo_rf = x_test_promo_pca[x_test_promo_pca.select_dtypes(include=['int', 'float', 'bool']).columns]\n",
    "\n",
    "x_val_normal_rf = x_val_normal_pca[x_val_normal_pca.select_dtypes(include=['int', 'float', 'bool']).columns]\n",
    "x_val_promo_rf = x_val_promo_pca[x_val_promo_pca.select_dtypes(include=['int', 'float', 'bool']).columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter tuning: n_estimators = 1\n",
      "Hyperparameter tuning: n_estimators = 3\n",
      "Hyperparameter tuning: n_estimators = 5\n",
      "Hyperparameter tuning: n_estimators = 7\n",
      "Hyperparameter tuning: n_estimators = 9\n",
      "Hyperparameter tuning: n_estimators = 1\n",
      "Hyperparameter tuning: n_estimators = 3\n",
      "Hyperparameter tuning: n_estimators = 5\n",
      "Hyperparameter tuning: n_estimators = 7\n",
      "Hyperparameter tuning: n_estimators = 9\n"
     ]
    }
   ],
   "source": [
    "# evaluate the rf model\n",
    "def evaluate_rf(X_train, y_train, X_val, y_val, X_test, y_test, std = True):\n",
    "\n",
    "    X_train_rf = X_train\n",
    "    X_val_rf = X_val\n",
    "    X_test_rf = X_test\n",
    "    \n",
    "    ## hyperparameter tuning\n",
    "    best_rmse = float('inf')\n",
    "    best_n_estimators = 0\n",
    "    for i in np.arange(1, 10, 2):\n",
    "        print(f\"Hyperparameter tuning: n_estimators = {i}\")\n",
    "        rf = RandomForestRegressor(n_estimators=i, random_state=3600)\n",
    "        rf.fit(X_train_rf, y_train)\n",
    "        y_val_pred = rf.predict(X_val_rf)\n",
    "        val_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
    "        if val_rmse < best_rmse:\n",
    "            best_rmse = val_rmse\n",
    "            best_n_estimators = i\n",
    "    \n",
    "    ## best model\n",
    "    best_rf = RandomForestRegressor(n_estimators=best_n_estimators, random_state=3600)\n",
    "    best_rf.fit(X_train_rf, y_train)\n",
    "    \n",
    "    ## evaluate on test set\n",
    "    y_test_pred = best_rf.predict(X_test_rf)\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "    \n",
    "    return best_rf, val_rmse, test_rmse\n",
    "\n",
    "\n",
    "rf_normal, val_rmse_normal_rf, test_rmse_normal_rf = evaluate_rf(x_train_normal_rf, y_train_normal, x_val_normal_rf, y_val_normal, x_test_normal_rf, y_test_normal)\n",
    "rf_promo, val_rmse_promo_rf, test_rmse_promo_rf = evaluate_rf(x_train_promo_rf, y_train_promo, x_val_promo_rf, y_val_promo, x_test_promo_rf, y_test_promo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Best Model Parameters</th>\n",
       "      <th>Top 10 Important Features</th>\n",
       "      <th>Validation RMSE</th>\n",
       "      <th>Test RMSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RF for Normal Sales</td>\n",
       "      <td>9</td>\n",
       "      <td>[sales_amount, sales_amount_exclude_gst, sell_price, cost_of_purchasing, net_profit, gross_profit, brandtype_Manufacturer Brand, subcategory_Soaps &amp; Wash, segment_Bar Soap, brandtype_National Brand]</td>\n",
       "      <td>0.052532</td>\n",
       "      <td>0.081294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RF for Promo Sales</td>\n",
       "      <td>9</td>\n",
       "      <td>[promo_units, cost_of_purchasing, brandtype_National Brand, sales_amount_exclude_gst, sell_price, sales_amount, promo_price, gross_profit, marginal_sales_per_dollar_decrease, segment_Huggies Essentials]</td>\n",
       "      <td>0.092181</td>\n",
       "      <td>0.103077</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model  Best Model Parameters  \\\n",
       "0  RF for Normal Sales                      9   \n",
       "1   RF for Promo Sales                      9   \n",
       "\n",
       "                                                                                                                                                                                    Top 10 Important Features  \\\n",
       "0      [sales_amount, sales_amount_exclude_gst, sell_price, cost_of_purchasing, net_profit, gross_profit, brandtype_Manufacturer Brand, subcategory_Soaps & Wash, segment_Bar Soap, brandtype_National Brand]   \n",
       "1  [promo_units, cost_of_purchasing, brandtype_National Brand, sales_amount_exclude_gst, sell_price, sales_amount, promo_price, gross_profit, marginal_sales_per_dollar_decrease, segment_Huggies Essentials]   \n",
       "\n",
       "   Validation RMSE  Test RMSE  \n",
       "0         0.052532   0.081294  \n",
       "1         0.092181   0.103077  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# important features\n",
    "important_features_normal = pd.DataFrame({'Feature': x_train_normal_rf.columns, 'Importance': rf_normal.feature_importances_})\n",
    "important_features_normal = important_features_normal.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "important_features_promo = pd.DataFrame({'Feature': x_train_promo_rf.columns, 'Importance': rf_promo.feature_importances_})\n",
    "important_features_promo = important_features_promo.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# format the output in a table\n",
    "output = pd.DataFrame({'Model': ['RF for Normal Sales', 'RF for Promo Sales'],\n",
    "                       'Best Model Parameters': [rf_normal.n_estimators, rf_promo.n_estimators],\n",
    "                       'Top 10 Important Features': [important_features_normal['Feature'].head(10).values, important_features_promo['Feature'].head(10).values],\n",
    "                       'Validation RMSE': [val_rmse_normal_rf, val_rmse_promo_rf],\n",
    "                       'Test RMSE': [test_rmse_normal_rf, test_rmse_promo_rf]})\n",
    "                      \n",
    "pd.set_option('display.max_colwidth', None)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Model Evaluation of the Best Model (Random Forest Regressor)\n",
    "- Destandardise the error so that it is interpretable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter tuning: n_estimators = 1\n",
      "Hyperparameter tuning: n_estimators = 3\n",
      "Hyperparameter tuning: n_estimators = 5\n",
      "Hyperparameter tuning: n_estimators = 7\n",
      "Hyperparameter tuning: n_estimators = 9\n",
      "Hyperparameter tuning: n_estimators = 1\n",
      "Hyperparameter tuning: n_estimators = 3\n",
      "Hyperparameter tuning: n_estimators = 5\n",
      "Hyperparameter tuning: n_estimators = 7\n",
      "Hyperparameter tuning: n_estimators = 9\n"
     ]
    }
   ],
   "source": [
    "y_val_normal_unstd, x_val_normal_unstd = y_x_split(val_normal_unstd)\n",
    "y_val_promo_unstd, x_val_promo_unstd = y_x_split(val_promo_unstd)\n",
    "y_test_normal_unstd, x_test_normal_unstd = y_x_split(test_normal_unstd)\n",
    "y_test_promo_unstd, x_test_promo_unstd = y_x_split(test_promo_unstd)\n",
    "\n",
    "\n",
    "rf_normal, val_rmse_normal_rf, test_rmse_normal_rf = evaluate_rf(x_train_normal_rf, y_train_normal, x_val_normal_rf, y_val_normal_unstd, x_test_normal_rf, y_test_normal_unstd)\n",
    "rf_promo, val_rmse_promo_rf, test_rmse_promo_rf = evaluate_rf(x_train_promo_rf, y_train_promo, x_val_promo_rf, y_val_promo_unstd, x_test_promo_rf, y_test_promo_unstd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Best Model Parameters</th>\n",
       "      <th>Top 10 Important Features</th>\n",
       "      <th>Validation RMSE</th>\n",
       "      <th>Test RMSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RF for Normal Sales</td>\n",
       "      <td>3</td>\n",
       "      <td>[sales_amount, sales_amount_exclude_gst, sell_price, cost_of_purchasing, net_profit, brandtype_Manufacturer Brand, gross_profit, day_of_week_5, segment_Bar Soap, brandtype_National Brand]</td>\n",
       "      <td>61.975598</td>\n",
       "      <td>55.292358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RF for Promo Sales</td>\n",
       "      <td>9</td>\n",
       "      <td>[promo_units, cost_of_purchasing, brandtype_National Brand, sales_amount_exclude_gst, sell_price, sales_amount, promo_price, gross_profit, marginal_sales_per_dollar_decrease, segment_Huggies Essentials]</td>\n",
       "      <td>94.587343</td>\n",
       "      <td>88.887927</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model  Best Model Parameters  \\\n",
       "0  RF for Normal Sales                      3   \n",
       "1   RF for Promo Sales                      9   \n",
       "\n",
       "                                                                                                                                                                                    Top 10 Important Features  \\\n",
       "0                 [sales_amount, sales_amount_exclude_gst, sell_price, cost_of_purchasing, net_profit, brandtype_Manufacturer Brand, gross_profit, day_of_week_5, segment_Bar Soap, brandtype_National Brand]   \n",
       "1  [promo_units, cost_of_purchasing, brandtype_National Brand, sales_amount_exclude_gst, sell_price, sales_amount, promo_price, gross_profit, marginal_sales_per_dollar_decrease, segment_Huggies Essentials]   \n",
       "\n",
       "   Validation RMSE  Test RMSE  \n",
       "0        61.975598  55.292358  \n",
       "1        94.587343  88.887927  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# important features\n",
    "important_features_normal = pd.DataFrame({'Feature': x_train_normal_rf.columns, 'Importance': rf_normal.feature_importances_})\n",
    "important_features_normal = important_features_normal.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "important_features_promo = pd.DataFrame({'Feature': x_train_promo_rf.columns, 'Importance': rf_promo.feature_importances_})\n",
    "important_features_promo = important_features_promo.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# format the output in a table\n",
    "output = pd.DataFrame({'Model': ['RF for Normal Sales', 'RF for Promo Sales'],\n",
    "                       'Best Model Parameters': [rf_normal.n_estimators, rf_promo.n_estimators],\n",
    "                       'Top 10 Important Features': [important_features_normal['Feature'].head(10).values, important_features_promo['Feature'].head(10).values],\n",
    "                       'Validation RMSE': [val_rmse_normal_rf, val_rmse_promo_rf],\n",
    "                       'Test RMSE': [test_rmse_normal_rf, test_rmse_promo_rf]})\n",
    "                      \n",
    "pd.set_option('display.max_colwidth', None)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Bundle Creation\n",
    "- Promo is more prone to change the sales units than Normal Sales, hence even though promo sale RF model has a lower RMSE, we will use the Promo Sales model to create the bundles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>subcategory_Cleaning Products</td>\n",
       "      <td>7.254915e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>subcategory_Baby Wipes</td>\n",
       "      <td>2.786195e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>subcategory_Skincare Body</td>\n",
       "      <td>2.773451e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>subcategory_Laundry Detergent</td>\n",
       "      <td>1.468540e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>subcategory_Deodorants</td>\n",
       "      <td>1.098158e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>subcategory_Prewash &amp; Conditio</td>\n",
       "      <td>7.920214e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>subcategory_Disposable Nappies</td>\n",
       "      <td>3.196743e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>subcategory_Oral Care</td>\n",
       "      <td>3.115163e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>subcategory_Baby Toiletries</td>\n",
       "      <td>2.854176e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>subcategory_Baby Food</td>\n",
       "      <td>1.927799e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>subcategory_Soaps &amp; Wash</td>\n",
       "      <td>1.658594e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>subcategory_Dishwash</td>\n",
       "      <td>1.157915e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>subcategory_Mopping</td>\n",
       "      <td>1.122731e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>subcategory_Baby Formula</td>\n",
       "      <td>1.039745e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>subcategory_Skincare Face</td>\n",
       "      <td>9.340288e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>subcategory_Lip</td>\n",
       "      <td>7.659381e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>subcategory_Bags &amp; Wraps</td>\n",
       "      <td>3.040838e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>subcategory_Sunscreen</td>\n",
       "      <td>1.745958e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>subcategory_Tanning</td>\n",
       "      <td>3.715720e-08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Feature    Importance\n",
       "11   subcategory_Cleaning Products  7.254915e-04\n",
       "65          subcategory_Baby Wipes  2.786195e-04\n",
       "10       subcategory_Skincare Body  2.773451e-05\n",
       "15   subcategory_Laundry Detergent  1.468540e-05\n",
       "58          subcategory_Deodorants  1.098158e-05\n",
       "39  subcategory_Prewash & Conditio  7.920214e-06\n",
       "7   subcategory_Disposable Nappies  3.196743e-06\n",
       "50           subcategory_Oral Care  3.115163e-06\n",
       "17     subcategory_Baby Toiletries  2.854176e-06\n",
       "8            subcategory_Baby Food  1.927799e-06\n",
       "68        subcategory_Soaps & Wash  1.658594e-06\n",
       "26            subcategory_Dishwash  1.157915e-06\n",
       "72             subcategory_Mopping  1.122731e-06\n",
       "66        subcategory_Baby Formula  1.039745e-06\n",
       "5        subcategory_Skincare Face  9.340288e-07\n",
       "31                 subcategory_Lip  7.659381e-07\n",
       "81        subcategory_Bags & Wraps  3.040838e-07\n",
       "12           subcategory_Sunscreen  1.745958e-07\n",
       "47             subcategory_Tanning  3.715720e-08"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "all_important_features = rf_promo.feature_importances_\n",
    "\n",
    "# get the features variables with the importance\n",
    "pd.set_option('display.max_rows', None)\n",
    "important_features = pd.DataFrame({'Feature': x_train_promo_rf.columns, 'Importance': all_important_features})\n",
    "important_features\n",
    "\n",
    "# see only the subcategory\n",
    "pd.set_option('display.max_rows', None)\n",
    "subcat = important_features[important_features['Feature'].str.contains('subcategory')]\n",
    "\n",
    "# rank by importance\n",
    "pd.set_option('display.max_rows', None)\n",
    "subcat = subcat.sort_values(by='Importance', ascending=False)\n",
    "subcat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Bundle Combination (Subcategory)\n",
    "- Using the promo sales model, extracting the top 3 subcat and making combinations with the rest of the subcats to create bundles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the top 3 subcategories based on rf importance as the top sales drivers\n",
    "top3_subcat = subcat.head(3)\n",
    "\n",
    "# other subcategories\n",
    "other_subcat = subcat.tail(subcat.shape[0] - 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Top Subcategory</th>\n",
       "      <th>Other Subcategory</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>subcategory_Cleaning Products</td>\n",
       "      <td>subcategory_Laundry Detergent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>subcategory_Cleaning Products</td>\n",
       "      <td>subcategory_Deodorants</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>subcategory_Cleaning Products</td>\n",
       "      <td>subcategory_Prewash &amp; Conditio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>subcategory_Cleaning Products</td>\n",
       "      <td>subcategory_Disposable Nappies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>subcategory_Cleaning Products</td>\n",
       "      <td>subcategory_Oral Care</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Top Subcategory               Other Subcategory\n",
       "0  subcategory_Cleaning Products   subcategory_Laundry Detergent\n",
       "1  subcategory_Cleaning Products          subcategory_Deodorants\n",
       "2  subcategory_Cleaning Products  subcategory_Prewash & Conditio\n",
       "3  subcategory_Cleaning Products  subcategory_Disposable Nappies\n",
       "4  subcategory_Cleaning Products           subcategory_Oral Care"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine the top 3 subcategories and other subcategories\n",
    "bundle_data = pd.DataFrame()\n",
    "bundle_data['Top Subcategory'] = np.repeat(top3_subcat['Feature'], len(other_subcat))\n",
    "bundle_data['Other Subcategory'] = np.tile(other_subcat['Feature'], len(top3_subcat))\n",
    "\n",
    "bundle_data = bundle_data.reset_index(drop=True)\n",
    "print(bundle_data.shape)\n",
    "bundle_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Generate Bundles ID\n",
    "- creating unique article id for each bundle, treating as a new article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Top Subcategory</th>\n",
       "      <th>Other Subcategory</th>\n",
       "      <th>bundle_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>subcategory_Cleaning Products</td>\n",
       "      <td>subcategory_Laundry Detergent</td>\n",
       "      <td>430030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>subcategory_Cleaning Products</td>\n",
       "      <td>subcategory_Deodorants</td>\n",
       "      <td>505208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>subcategory_Cleaning Products</td>\n",
       "      <td>subcategory_Prewash &amp; Conditio</td>\n",
       "      <td>220109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>subcategory_Cleaning Products</td>\n",
       "      <td>subcategory_Disposable Nappies</td>\n",
       "      <td>392445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>subcategory_Cleaning Products</td>\n",
       "      <td>subcategory_Oral Care</td>\n",
       "      <td>285107</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Top Subcategory               Other Subcategory  bundle_id\n",
       "0  subcategory_Cleaning Products   subcategory_Laundry Detergent     430030\n",
       "1  subcategory_Cleaning Products          subcategory_Deodorants     505208\n",
       "2  subcategory_Cleaning Products  subcategory_Prewash & Conditio     220109\n",
       "3  subcategory_Cleaning Products  subcategory_Disposable Nappies     392445\n",
       "4  subcategory_Cleaning Products           subcategory_Oral Care     285107"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create new article id for the bundle data\n",
    "def create_bundle_id(bundle_data, original_data):\n",
    "    # generate a new article id for the bundle data that's not in the original data\n",
    "    existing_id = original_data['article_id'].unique()\n",
    "    bundle_data['bundle_id'] = [0]*bundle_data.shape[0]\n",
    "    \n",
    "    # create a new article id for the bundle data\n",
    "    np.random.seed(3600)\n",
    "    for idx in bundle_data.index:\n",
    "        new_id = np.random.randint(100000, 999999)\n",
    "        while new_id in existing_id:\n",
    "            new_id = np.random.randint(100000, 999999)\n",
    "            existing_id.append(new_id)\n",
    "        bundle_data['bundle_id'][idx] = new_id\n",
    "            \n",
    "    return bundle_data\n",
    "\n",
    "bundle_data = create_bundle_id(bundle_data, data_train_with_dummy)\n",
    "bundle_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Select the specific articles for the bundles\n",
    "- Top 3 subcategories - 3 articles that has the highest median sales units under the subcategory\n",
    "- Rest of the subcategories - choose the least median sales units article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48, 7)\n"
     ]
    }
   ],
   "source": [
    "def select_bundle_article(bundle_data, original_data):\n",
    "    for idx in bundle_data.index:\n",
    "        top_cat = bundle_data['Top Subcategory'][idx]\n",
    "        other_cat = bundle_data['Other Subcategory'][idx]\n",
    "        \n",
    "        # compute the median sales units for the top category\n",
    "        top_cat = original_data[original_data[top_cat] == 1]\n",
    "        # for each article in the top category, compute the median sales units\n",
    "        top_cat_median = top_cat.groupby('article_id')['sales_units'].median()\n",
    "        # rank from highest to lowest\n",
    "        top_cat_median = top_cat_median.sort_values(ascending=False)\n",
    "        # make sure the top article contain promo price\n",
    "        for article_id in top_cat_median.index:\n",
    "            # check if the article's promo price is all na\n",
    "            if original_data[original_data['article_id'] == article_id]['promo_price'].isna().all():\n",
    "                continue\n",
    "            else:\n",
    "                top_article_id = article_id\n",
    "                break\n",
    "        # add the article id to the bundle data\n",
    "        bundle_data.loc[idx, 'Top Article ID'] = top_article_id\n",
    "        # add description\n",
    "        bundle_data.loc[idx, 'Top Article Description'] = original_data[original_data['article_id'] == top_article_id]['article_desc'].values[0]\n",
    "        \n",
    "        # compute the median sales units for the other category\n",
    "        other_cat = original_data[original_data[other_cat] == 1]\n",
    "        # for each article in the other category, compute the median sales units\n",
    "        other_cat_median = other_cat.groupby('article_id')['sales_units'].median()\n",
    "        # rank from lowest to highest\n",
    "        other_cat_median = other_cat_median.sort_values(ascending=True)\n",
    "        # make sure the other article contain promo price\n",
    "        for article_id in other_cat_median.index:\n",
    "            # check if the article's promo price is all na\n",
    "            if original_data[original_data['article_id'] == article_id]['promo_price'].isna().all():\n",
    "                continue\n",
    "            # check if the sell units is 0\n",
    "            elif original_data[original_data['article_id'] == article_id]['sales_units'].median() == 0:\n",
    "                continue\n",
    "            else:\n",
    "                other_article_id = article_id\n",
    "                break\n",
    "        # add the article id to the bundle data\n",
    "        bundle_data.loc[idx, 'Other Article ID'] = other_article_id\n",
    "        # add description\n",
    "        bundle_data.loc[idx, 'Other Article Description'] = original_data[original_data['article_id'] == other_article_id]['article_desc'].values[0]\n",
    "        \n",
    "    return bundle_data\n",
    "        \n",
    "        \n",
    "bundle_data = select_bundle_article(bundle_data, data_train_with_dummy)\n",
    "print(bundle_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(733589, 172)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train_with_dummy.shape\n",
    "x_train_normal_rf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High Performing Article: Strike Disinfectant Wipes Lemon 100pk with median sales units: 476.0\n",
      "Low Performing Article: Fab Subblime Velvet 15kg with median sales units: 1.0\n",
      "=====================END OF THIS BUNDLE================================\n",
      "High Performing Article: Strike Disinfectant Wipes Lemon 100pk with median sales units: 476.0\n",
      "Low Performing Article: Dove Powder Soft Roll On 50ml with median sales units: 1.0\n",
      "=====================END OF THIS BUNDLE================================\n",
      "High Performing Article: Strike Disinfectant Wipes Lemon 100pk with median sales units: 476.0\n",
      "Low Performing Article: Vanish Gold Pro OxiAction White2.7kg with median sales units: 1.0\n",
      "=====================END OF THIS BUNDLE================================\n",
      "High Performing Article: Strike Disinfectant Wipes Lemon 100pk with median sales units: 476.0\n",
      "Low Performing Article: Hug UDPant Size 3 Crawler  Boy 36 Bulk with median sales units: 8.0\n",
      "=====================END OF THIS BUNDLE================================\n",
      "High Performing Article: Strike Disinfectant Wipes Lemon 100pk with median sales units: 476.0\n",
      "Low Performing Article: Colgate Infinity Starter Kit T/Brush 1pk with median sales units: 1.0\n",
      "=====================END OF THIS BUNDLE================================\n",
      "High Performing Article: Strike Disinfectant Wipes Lemon 100pk with median sales units: 476.0\n",
      "Low Performing Article: TT Made Fof Me Breast Pads - M (40PK) with median sales units: 1.0\n",
      "=====================END OF THIS BUNDLE================================\n",
      "High Performing Article: Strike Disinfectant Wipes Lemon 100pk with median sales units: 476.0\n",
      "Low Performing Article: Nestle Cerelac      Rice 4m 200g with median sales units: 1.0\n",
      "=====================END OF THIS BUNDLE================================\n",
      "High Performing Article: Strike Disinfectant Wipes Lemon 100pk with median sales units: 476.0\n",
      "Low Performing Article: PalmoliveHWashLiq   SkinFoodPeach500ml with median sales units: 1.0\n",
      "=====================END OF THIS BUNDLE================================\n",
      "High Performing Article: Strike Disinfectant Wipes Lemon 100pk with median sales units: 476.0\n",
      "Low Performing Article: Finish Ultimate Pro Dish Tabs Lemon 32pk with median sales units: 3.0\n",
      "=====================END OF THIS BUNDLE================================\n",
      "High Performing Article: Strike Disinfectant Wipes Lemon 100pk with median sales units: 476.0\n",
      "Low Performing Article: Sabco Lightng Mop   Squeeze Scourer with median sales units: 2.0\n",
      "=====================END OF THIS BUNDLE================================\n",
      "High Performing Article: Strike Disinfectant Wipes Lemon 100pk with median sales units: 476.0\n",
      "Low Performing Article: NAN SUPREMEPRO Stage 4 800g with median sales units: 7.0\n",
      "=====================END OF THIS BUNDLE================================\n",
      "High Performing Article: Strike Disinfectant Wipes Lemon 100pk with median sales units: 476.0\n",
      "Low Performing Article: NBN Bioactive Rosehip Oil 30ml with median sales units: 0.5\n",
      "=====================END OF THIS BUNDLE================================\n",
      "High Performing Article: Strike Disinfectant Wipes Lemon 100pk with median sales units: 476.0\n",
      "Low Performing Article: Lipsmacker Frozen II Lip Gloss Pouch with median sales units: 6.0\n",
      "=====================END OF THIS BUNDLE================================\n",
      "High Performing Article: Strike Disinfectant Wipes Lemon 100pk with median sales units: 476.0\n",
      "Low Performing Article: Multix Alfoil       30cmX60m with median sales units: 17.0\n",
      "=====================END OF THIS BUNDLE================================\n",
      "High Performing Article: Strike Disinfectant Wipes Lemon 100pk with median sales units: 476.0\n",
      "Low Performing Article: C/Council Sunscrn   Sprt SPF50Pls500ml with median sales units: 1.0\n",
      "=====================END OF THIS BUNDLE================================\n",
      "High Performing Article: Strike Disinfectant Wipes Lemon 100pk with median sales units: 476.0\n",
      "Low Performing Article: Bondi Sands Pure Water Light/Med 200ml with median sales units: 1.0\n",
      "=====================END OF THIS BUNDLE================================\n",
      "High Performing Article: Dymples Wipes Fragrance Free 480pk with median sales units: 942.0\n",
      "Low Performing Article: Fab Subblime Velvet 15kg with median sales units: 1.0\n",
      "=====================END OF THIS BUNDLE================================\n",
      "High Performing Article: Dymples Wipes Fragrance Free 480pk with median sales units: 942.0\n",
      "Low Performing Article: Dove Powder Soft Roll On 50ml with median sales units: 1.0\n",
      "=====================END OF THIS BUNDLE================================\n",
      "High Performing Article: Dymples Wipes Fragrance Free 480pk with median sales units: 942.0\n",
      "Low Performing Article: Vanish Gold Pro OxiAction White2.7kg with median sales units: 1.0\n",
      "=====================END OF THIS BUNDLE================================\n",
      "High Performing Article: Dymples Wipes Fragrance Free 480pk with median sales units: 942.0\n",
      "Low Performing Article: Hug UDPant Size 3 Crawler  Boy 36 Bulk with median sales units: 8.0\n",
      "=====================END OF THIS BUNDLE================================\n",
      "High Performing Article: Dymples Wipes Fragrance Free 480pk with median sales units: 942.0\n",
      "Low Performing Article: Colgate Infinity Starter Kit T/Brush 1pk with median sales units: 1.0\n",
      "=====================END OF THIS BUNDLE================================\n",
      "High Performing Article: Dymples Wipes Fragrance Free 480pk with median sales units: 942.0\n",
      "Low Performing Article: TT Made Fof Me Breast Pads - M (40PK) with median sales units: 1.0\n",
      "=====================END OF THIS BUNDLE================================\n",
      "High Performing Article: Dymples Wipes Fragrance Free 480pk with median sales units: 942.0\n",
      "Low Performing Article: Nestle Cerelac      Rice 4m 200g with median sales units: 1.0\n",
      "=====================END OF THIS BUNDLE================================\n",
      "High Performing Article: Dymples Wipes Fragrance Free 480pk with median sales units: 942.0\n",
      "Low Performing Article: PalmoliveHWashLiq   SkinFoodPeach500ml with median sales units: 1.0\n",
      "=====================END OF THIS BUNDLE================================\n",
      "High Performing Article: Dymples Wipes Fragrance Free 480pk with median sales units: 942.0\n",
      "Low Performing Article: Finish Ultimate Pro Dish Tabs Lemon 32pk with median sales units: 3.0\n",
      "=====================END OF THIS BUNDLE================================\n",
      "High Performing Article: Dymples Wipes Fragrance Free 480pk with median sales units: 942.0\n",
      "Low Performing Article: Sabco Lightng Mop   Squeeze Scourer with median sales units: 2.0\n",
      "=====================END OF THIS BUNDLE================================\n",
      "High Performing Article: Dymples Wipes Fragrance Free 480pk with median sales units: 942.0\n",
      "Low Performing Article: NAN SUPREMEPRO Stage 4 800g with median sales units: 7.0\n",
      "=====================END OF THIS BUNDLE================================\n",
      "High Performing Article: Dymples Wipes Fragrance Free 480pk with median sales units: 942.0\n",
      "Low Performing Article: NBN Bioactive Rosehip Oil 30ml with median sales units: 0.5\n",
      "=====================END OF THIS BUNDLE================================\n",
      "High Performing Article: Dymples Wipes Fragrance Free 480pk with median sales units: 942.0\n",
      "Low Performing Article: Lipsmacker Frozen II Lip Gloss Pouch with median sales units: 6.0\n",
      "=====================END OF THIS BUNDLE================================\n",
      "High Performing Article: Dymples Wipes Fragrance Free 480pk with median sales units: 942.0\n",
      "Low Performing Article: Multix Alfoil       30cmX60m with median sales units: 17.0\n",
      "=====================END OF THIS BUNDLE================================\n",
      "High Performing Article: Dymples Wipes Fragrance Free 480pk with median sales units: 942.0\n",
      "Low Performing Article: C/Council Sunscrn   Sprt SPF50Pls500ml with median sales units: 1.0\n",
      "=====================END OF THIS BUNDLE================================\n",
      "High Performing Article: Dymples Wipes Fragrance Free 480pk with median sales units: 942.0\n",
      "Low Performing Article: Bondi Sands Pure Water Light/Med 200ml with median sales units: 1.0\n",
      "=====================END OF THIS BUNDLE================================\n",
      "High Performing Article: Australian Creams   Lanolin 250g with median sales units: 206.0\n",
      "Low Performing Article: Fab Subblime Velvet 15kg with median sales units: 1.0\n",
      "=====================END OF THIS BUNDLE================================\n",
      "High Performing Article: Australian Creams   Lanolin 250g with median sales units: 206.0\n",
      "Low Performing Article: Dove Powder Soft Roll On 50ml with median sales units: 1.0\n",
      "=====================END OF THIS BUNDLE================================\n",
      "High Performing Article: Australian Creams   Lanolin 250g with median sales units: 206.0\n",
      "Low Performing Article: Vanish Gold Pro OxiAction White2.7kg with median sales units: 1.0\n",
      "=====================END OF THIS BUNDLE================================\n",
      "High Performing Article: Australian Creams   Lanolin 250g with median sales units: 206.0\n",
      "Low Performing Article: Hug UDPant Size 3 Crawler  Boy 36 Bulk with median sales units: 8.0\n",
      "=====================END OF THIS BUNDLE================================\n",
      "High Performing Article: Australian Creams   Lanolin 250g with median sales units: 206.0\n",
      "Low Performing Article: Colgate Infinity Starter Kit T/Brush 1pk with median sales units: 1.0\n",
      "=====================END OF THIS BUNDLE================================\n",
      "High Performing Article: Australian Creams   Lanolin 250g with median sales units: 206.0\n",
      "Low Performing Article: TT Made Fof Me Breast Pads - M (40PK) with median sales units: 1.0\n",
      "=====================END OF THIS BUNDLE================================\n",
      "High Performing Article: Australian Creams   Lanolin 250g with median sales units: 206.0\n",
      "Low Performing Article: Nestle Cerelac      Rice 4m 200g with median sales units: 1.0\n",
      "=====================END OF THIS BUNDLE================================\n",
      "High Performing Article: Australian Creams   Lanolin 250g with median sales units: 206.0\n",
      "Low Performing Article: PalmoliveHWashLiq   SkinFoodPeach500ml with median sales units: 1.0\n",
      "=====================END OF THIS BUNDLE================================\n",
      "High Performing Article: Australian Creams   Lanolin 250g with median sales units: 206.0\n",
      "Low Performing Article: Finish Ultimate Pro Dish Tabs Lemon 32pk with median sales units: 3.0\n",
      "=====================END OF THIS BUNDLE================================\n",
      "High Performing Article: Australian Creams   Lanolin 250g with median sales units: 206.0\n",
      "Low Performing Article: Sabco Lightng Mop   Squeeze Scourer with median sales units: 2.0\n",
      "=====================END OF THIS BUNDLE================================\n",
      "High Performing Article: Australian Creams   Lanolin 250g with median sales units: 206.0\n",
      "Low Performing Article: NAN SUPREMEPRO Stage 4 800g with median sales units: 7.0\n",
      "=====================END OF THIS BUNDLE================================\n",
      "High Performing Article: Australian Creams   Lanolin 250g with median sales units: 206.0\n",
      "Low Performing Article: NBN Bioactive Rosehip Oil 30ml with median sales units: 0.5\n",
      "=====================END OF THIS BUNDLE================================\n",
      "High Performing Article: Australian Creams   Lanolin 250g with median sales units: 206.0\n",
      "Low Performing Article: Lipsmacker Frozen II Lip Gloss Pouch with median sales units: 6.0\n",
      "=====================END OF THIS BUNDLE================================\n",
      "High Performing Article: Australian Creams   Lanolin 250g with median sales units: 206.0\n",
      "Low Performing Article: Multix Alfoil       30cmX60m with median sales units: 17.0\n",
      "=====================END OF THIS BUNDLE================================\n",
      "High Performing Article: Australian Creams   Lanolin 250g with median sales units: 206.0\n",
      "Low Performing Article: C/Council Sunscrn   Sprt SPF50Pls500ml with median sales units: 1.0\n",
      "=====================END OF THIS BUNDLE================================\n",
      "High Performing Article: Australian Creams   Lanolin 250g with median sales units: 206.0\n",
      "Low Performing Article: Bondi Sands Pure Water Light/Med 200ml with median sales units: 1.0\n",
      "=====================END OF THIS BUNDLE================================\n"
     ]
    }
   ],
   "source": [
    "for i in bundle_data.index:\n",
    "    row = bundle_data.loc[i]\n",
    "    high_id = row['Top Article ID']\n",
    "    high_id_desc = row['Top Article Description']\n",
    "    high_id_median_sales_unit = data_train_with_dummy[data_train_with_dummy['article_id'] == high_id]['sales_units'].median()\n",
    "    low_id = row['Other Article ID']\n",
    "    low_id_desc = row['Other Article Description']\n",
    "    low_id_median_sales_unit = data_train_with_dummy[data_train_with_dummy['article_id'] == low_id]['sales_units'].median()\n",
    "    print(f\"High Performing Article: {high_id_desc} with median sales units: {high_id_median_sales_unit}\")\n",
    "    print(f\"Low Performing Article: {low_id_desc} with median sales units: {low_id_median_sales_unit}\")\n",
    "    print(\"=====================END OF THIS BUNDLE================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 Create the bundles dataset\n",
    "- Create features for the bundles dataset (matching the features in the original dataset)\n",
    "    - in preparation creation of the dummy variables\n",
    "    - then eases the fitting of the rf model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48, 8)\n"
     ]
    }
   ],
   "source": [
    "def add_bundle_dates(bundle_data, original_data):\n",
    "    # add the dates in which the top article is not on promotion\n",
    "    bundle_data['Bundle Dates'] = [0]*bundle_data.shape[0]\n",
    "    total_calendar_days = 0\n",
    "    for idx in bundle_data.index:\n",
    "        ## get the top article id non-promo dates\n",
    "        top_article_id = bundle_data['Top Article ID'][idx]\n",
    "        top_article = original_data[original_data['article_id'] == top_article_id]\n",
    "        top_article_non_promo_dates = top_article[top_article['promo_price'].isna()]['calendar_day']\n",
    "        top_article_non_promo_dates = top_article_non_promo_dates.tolist()\n",
    "        \n",
    "        ## get the other article id non-promo dates\n",
    "        bottom_article_id = bundle_data['Other Article ID'][idx]\n",
    "        bottom_article = original_data[original_data['article_id'] == bottom_article_id]\n",
    "        bottom_article_non_promo_dates = bottom_article[bottom_article['promo_price'].isna()]['calendar_day']\n",
    "        bottom_article_non_promo_dates = bottom_article_non_promo_dates.tolist()\n",
    "        \n",
    "        ## get the intersection of the non-promo dates\n",
    "        non_promo_dates = list(set(top_article_non_promo_dates).intersection(bottom_article_non_promo_dates))\n",
    "        bundle_data['Bundle Dates'][idx] = non_promo_dates\n",
    "        total_calendar_days += len(non_promo_dates)\n",
    "    return bundle_data, total_calendar_days\n",
    "\n",
    "bundle_data, total_calendar_days = add_bundle_dates(bundle_data, data_train_with_dummy)\n",
    "print(bundle_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_dummy_for_bundle(top_cat_df, bottom_cat_df, var_interested):\n",
    "    top_cat = top_cat_df[top_cat_df.columns[top_cat_df.columns.str.contains(var_interested)]].iloc[0]\n",
    "    bottom_cat = bottom_cat_df[bottom_cat_df.columns[bottom_cat_df.columns.str.contains(var_interested)]].iloc[0]\n",
    "    dummy_vars = top_cat | bottom_cat\n",
    "    return dummy_vars\n",
    "\n",
    "def dummies_to_bundle_df(top_cat_df, bottom_cat_df, var_interested, num_data):\n",
    "    combined_dummies = combine_dummy_for_bundle(top_cat_df, bottom_cat_df, var_interested)\n",
    "    combined_dummies = pd.DataFrame(combined_dummies).T\n",
    "    # repeat the combined dummies for the number of rows in the bundle data\n",
    "    combined_dummies = pd.concat([combined_dummies]*num_data, ignore_index=True)\n",
    "    return combined_dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7028, 424)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_bundle_data(bundle_data, original_data):\n",
    "    # Create a new dataframe for the bundle data\n",
    "    bundle_specs_df = pd.DataFrame()\n",
    "    bundle_data_valid = bundle_data.copy()\n",
    "    \n",
    "    for idx in bundle_data.index:\n",
    "        # # for debugging\n",
    "        # print(bundle_data.shape)\n",
    "        # print(f\"Currently processing bundle {idx}\")\n",
    "        \n",
    "        # create a temporary dataframe to store each bundle's data\n",
    "        temp_df = pd.DataFrame()\n",
    "        bundle_id = bundle_data['bundle_id'][idx]\n",
    "        top_article_id = bundle_data['Top Article ID'][idx]\n",
    "        bottom_article_id = bundle_data['Other Article ID'][idx]\n",
    "        dates = bundle_data['Bundle Dates'][idx]\n",
    "        \n",
    "        top_article_id_info = original_data[original_data['article_id'] == top_article_id]\n",
    "        bottom_article_id_info = original_data[original_data['article_id'] == bottom_article_id]\n",
    "        \n",
    "        # first check if there exist promo price for both articles, if not, get rid of the bundle\n",
    "        if top_article_id_info['promo_price'].isna().all() or bottom_article_id_info['promo_price'].isna().all():\n",
    "            bundle_data_valid = bundle_data_valid.drop(idx)\n",
    "            continue\n",
    "        \n",
    "        # then check if gst flag is the same for both articles, if not, hard to determine gst flag for the bundle\n",
    "        if top_article_id_info['gst_flag_Y'].all() != bottom_article_id_info['gst_flag_Y'].all():\n",
    "            bundle_data_valid = bundle_data_valid.drop(idx)\n",
    "            continue\n",
    "            \n",
    "        ## add the calendar day for each bundle\n",
    "        temp_df['calendar_day'] = dates\n",
    "        \n",
    "        ## add the bundle id for each bundle\n",
    "        temp_df['article_id'] = bundle_id\n",
    "        \n",
    "        ## add the bundle description for each bundle\n",
    "        temp_df['article_desc'] = f\"{bundle_data['Top Article Description'][idx]} + {bundle_data['Other Article Description'][idx]}\"\n",
    "        \n",
    "        # add dummy variables for the bundle\n",
    "        ## 1. add the category and subcategory for each bundle\n",
    "        cat_dummies = dummies_to_bundle_df(top_article_id_info, bottom_article_id_info, 'category', len(dates))\n",
    "        temp_df = pd.concat([temp_df, cat_dummies], axis=1)\n",
    "        \n",
    "        ## 2. add the segment for each bundle\n",
    "        segment_dummies = dummies_to_bundle_df(top_article_id_info, bottom_article_id_info, 'segment', len(dates))\n",
    "        temp_df = pd.concat([temp_df, segment_dummies], axis=1)\n",
    "        \n",
    "        ## 3. add the brand and brandtype for each bundle\n",
    "        brand_dummies = dummies_to_bundle_df(top_article_id_info, bottom_article_id_info, 'brand', len(dates))\n",
    "        temp_df = pd.concat([temp_df, brand_dummies], axis=1)\n",
    "        \n",
    "        ## 4. add the sell price\n",
    "        ### lowest bundle sell price = top_article median(promo_price) + bottom_article median(promo_price)\n",
    "        ### highest bundle sell price = top_article median(sell_price) + bottom_article median(promo_price)\n",
    "        ### take the average of the two as the bundle sell price\n",
    "        top_article_median_promo_price = top_article_id_info['promo_price'].median()\n",
    "        top_article_median_sell_price = top_article_id_info['sell_price'].median()\n",
    "        bottom_article_median_promo_price = bottom_article_id_info['promo_price'].median()\n",
    "        \n",
    "        low_threshold = top_article_median_promo_price + bottom_article_median_promo_price\n",
    "        high_threshold = top_article_median_sell_price + bottom_article_median_promo_price\n",
    "        \n",
    "        bundle_price = (low_threshold + high_threshold) / 2\n",
    "        temp_df['sell_price'] = pd.concat([pd.Series([bundle_price]*len(dates))], ignore_index=True)\n",
    "        \n",
    "        ## 5. add the promo price (same as sell price)\n",
    "        temp_df['promo_price'] = temp_df['sell_price']\n",
    "        \n",
    "        ## 6. add the gst flag and convert to dummy\n",
    "        gst_flag = data_train[data_train['article_id'] == top_article_id]['gst_flag'].values[0]\n",
    "        temp_df['gst_flag'] = pd.concat([pd.Series([gst_flag]*len(dates))], ignore_index=True)\n",
    "        temp_df = cat_to_dum(temp_df, 'gst_flag')\n",
    "        ### all bundle articles contains the same gst flag 'Y', but for data completion, we add both columns\n",
    "        temp_df['gst_flag_N'] = 0\n",
    "        \n",
    "        ## 7. add scanback using the average of the median scanback of the top and bottom articles\n",
    "        top_article_median_scanback = top_article_id_info['scanback'].median()\n",
    "        bottom_article_median_scanback = bottom_article_id_info['scanback'].median()\n",
    "        bundle_scanback = (top_article_median_scanback + bottom_article_median_scanback) / 2\n",
    "        temp_df['scanback'] = pd.concat([pd.Series([bundle_scanback]*len(dates))], ignore_index=True)\n",
    "\n",
    "        ## 8. add the median promo sales of the articles and take the average\n",
    "        promo_sales = top_article_id_info['promo_sales'].median() + bottom_article_id_info['promo_sales'].median()\n",
    "        promo_sales = promo_sales / 2\n",
    "        temp_df['promo_sales'] = pd.concat([pd.Series([promo_sales]*len(dates))], ignore_index=True)\n",
    "        \n",
    "        ## 9. do the same for promo_units\n",
    "        promo_units = top_article_id_info['promo_units'].median() + bottom_article_id_info['promo_units'].median()\n",
    "        promo_units = promo_units / 2\n",
    "        temp_df['promo_units'] = pd.concat([pd.Series([promo_units]*len(dates))], ignore_index=True)\n",
    "        \n",
    "        ## 10. do the same for the gross profit\n",
    "        gross_profit = top_article_id_info['gross_profit'].median() + bottom_article_id_info['gross_profit'].median()\n",
    "        gross_profit = gross_profit / 2\n",
    "        temp_df['gross_profit'] = pd.concat([pd.Series([gross_profit]*len(dates))], ignore_index=True)\n",
    "        \n",
    "        ## 11. do the same for the sales_amount\n",
    "        sales_amount = top_article_id_info['sales_amount'].median() + bottom_article_id_info['sales_amount'].median()\n",
    "        sales_amount = sales_amount / 2\n",
    "        temp_df['sales_amount'] = pd.concat([pd.Series([sales_amount]*len(dates))], ignore_index=True)\n",
    "        \n",
    "        temp_df = new_var(temp_df)\n",
    "        \n",
    "        # concatenate the temporary dataframe to the bundle_specs_df\n",
    "        bundle_specs_df = pd.concat([bundle_specs_df, temp_df], ignore_index=True)\n",
    "    \n",
    "    return bundle_specs_df, bundle_data_valid\n",
    "\n",
    "bundle_specs_df, valid_bundle = create_bundle_data(bundle_data, data_train_with_dummy)\n",
    "bundle_specs_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sales_units']\n"
     ]
    }
   ],
   "source": [
    "# difference between the bundle columns and the original data columns\n",
    "diff = list(set(data_train_with_dummy.columns) - set(bundle_specs_df.columns))\n",
    "print(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# match the columns\n",
    "bundle_cols = bundle_specs_df.columns\n",
    "rf_cols = x_train_promo_rf.columns\n",
    "\n",
    "bundle_copy = bundle_specs_df.copy()\n",
    "bundle_copy = bundle_copy[rf_cols]\n",
    "\n",
    "# predict the sales units using rf (best model)\n",
    "best_model = rf_promo\n",
    "bundle_sales_units = best_model.predict(bundle_copy)\n",
    "\n",
    "# add the predicted sales units to the bundle data\n",
    "bundle_specs_df['sales_units'] = bundle_sales_units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7028, 425)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>sales_units</th>\n",
       "      <th>Top Article Description</th>\n",
       "      <th>Other Article Description</th>\n",
       "      <th>Bundle Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>430030</td>\n",
       "      <td>1174.383936</td>\n",
       "      <td>Strike Disinfectant Wipes Lemon 100pk</td>\n",
       "      <td>Fab Subblime Velvet 15kg</td>\n",
       "      <td>40.335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>441278</td>\n",
       "      <td>1172.983115</td>\n",
       "      <td>Strike Disinfectant Wipes Lemon 100pk</td>\n",
       "      <td>TT Made Fof Me Breast Pads - M (40PK)</td>\n",
       "      <td>13.015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>505208</td>\n",
       "      <td>1108.525784</td>\n",
       "      <td>Strike Disinfectant Wipes Lemon 100pk</td>\n",
       "      <td>Dove Powder Soft Roll On 50ml</td>\n",
       "      <td>5.415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>392445</td>\n",
       "      <td>1084.478383</td>\n",
       "      <td>Strike Disinfectant Wipes Lemon 100pk</td>\n",
       "      <td>Hug UDPant Size 3 Crawler  Boy 36 Bulk</td>\n",
       "      <td>13.910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>338104</td>\n",
       "      <td>1072.379254</td>\n",
       "      <td>Strike Disinfectant Wipes Lemon 100pk</td>\n",
       "      <td>Bondi Sands Pure Water Light/Med 200ml</td>\n",
       "      <td>18.365</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    article_id  sales_units                Top Article Description  \\\n",
       "17      430030  1174.383936  Strike Disinfectant Wipes Lemon 100pk   \n",
       "18      441278  1172.983115  Strike Disinfectant Wipes Lemon 100pk   \n",
       "21      505208  1108.525784  Strike Disinfectant Wipes Lemon 100pk   \n",
       "15      392445  1084.478383  Strike Disinfectant Wipes Lemon 100pk   \n",
       "10      338104  1072.379254  Strike Disinfectant Wipes Lemon 100pk   \n",
       "\n",
       "                 Other Article Description  Bundle Price  \n",
       "17                Fab Subblime Velvet 15kg        40.335  \n",
       "18   TT Made Fof Me Breast Pads - M (40PK)        13.015  \n",
       "21           Dove Powder Soft Roll On 50ml         5.415  \n",
       "15  Hug UDPant Size 3 Crawler  Boy 36 Bulk        13.910  \n",
       "10  Bondi Sands Pure Water Light/Med 200ml        18.365  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# order the bundle data by sales units\n",
    "bundle_specs_df_agg = bundle_specs_df.copy()\n",
    "\n",
    "# aggregate the calendar days in 2 weeks\n",
    "bundle_specs_df_agg['calendar_day'] = pd.to_datetime(bundle_specs_df_agg['calendar_day'])\n",
    "print(bundle_specs_df_agg.shape)\n",
    "\n",
    "# group by article_id and aggregate the sales units\n",
    "bundle_specs_df_agg = bundle_specs_df_agg.groupby('article_id').agg({'sales_units': 'sum'}).reset_index()\n",
    "bundle_specs_df_agg = bundle_specs_df_agg.sort_values(by='sales_units', ascending=False)\n",
    "\n",
    "# get the description of the top 5 bundles\n",
    "bundle_specs_df_agg['Top Article Description'] = [0]*bundle_specs_df_agg.shape[0]\n",
    "bundle_specs_df_agg['Other Article Description'] = [0]*bundle_specs_df_agg.shape[0]\n",
    "\n",
    "for idx in bundle_specs_df_agg.index:\n",
    "    article_id = bundle_specs_df_agg['article_id'][idx]\n",
    "    top_article_desc = bundle_data[bundle_data['bundle_id'] == article_id]['Top Article Description'].values[0]\n",
    "    other_article_desc = bundle_data[bundle_data['bundle_id'] == article_id]['Other Article Description'].values[0]\n",
    "    bundle_specs_df_agg.loc[idx, 'Top Article Description'] = top_article_desc\n",
    "    bundle_specs_df_agg.loc[idx, 'Other Article Description'] = other_article_desc\n",
    "    bundle_specs_df_agg.loc[idx, 'Bundle Price'] = bundle_specs_df[bundle_specs_df['article_id'] == article_id]['sell_price'].values[0]\n",
    "    \n",
    "bundle_specs_df_agg.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Evaluate the performance of bundles\n",
    "- Apply the bundle discount to the low performing articles\n",
    "- Compare the performance of the bigW sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sales Before Bundle Applied: 26686855 units\n"
     ]
    }
   ],
   "source": [
    "# sales before price change\n",
    "sales_before = data_train_with_dummy['sales_units'].sum()\n",
    "print(f\"Total Sales Before Bundle Applied: {sales_before} units\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[430030,\n",
       " 441278,\n",
       " 505208,\n",
       " 392445,\n",
       " 338104,\n",
       " 220109,\n",
       " 611760,\n",
       " 340350,\n",
       " 500238,\n",
       " 820695]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# obtain top 10 bundles\n",
    "top_10_bundles_id = bundle_specs_df_agg.head(10)['article_id']\n",
    "top_10_bundles_id = top_10_bundles_id.tolist()\n",
    "top_10_bundles_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26689946.21945626"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mimick bundle sales by applying the total price change on the low performing article\n",
    "bundle_impact = bundle_specs_df.copy()\n",
    "data_train_with_dummy_after_bundle = data_train_with_dummy.copy()\n",
    "best_model = rf_promo\n",
    "\n",
    "# select the top 10 bundles\n",
    "bundle_impact = bundle_impact[bundle_impact['article_id'].isin(top_10_bundles_id)]\n",
    "\n",
    "# get the sell price of the top and low performing articles on that day\n",
    "bundle_impact['top_sell_price'] = [0]*bundle_impact.shape[0]\n",
    "bundle_impact['other_sell_price'] = [0]*bundle_impact.shape[0]\n",
    "\n",
    "for idx in bundle_impact.index:\n",
    "    # find the bundle id for top and low performing articles\n",
    "    bundle_id = bundle_impact['article_id'][idx]\n",
    "    top_article_id = valid_bundle[valid_bundle['bundle_id'] == bundle_id]['Top Article ID'].values[0]\n",
    "    other_article_id = valid_bundle[valid_bundle['bundle_id'] == bundle_id]['Other Article ID'].values[0]\n",
    "    \n",
    "    # find the bundle sell price\n",
    "    bundle_sell_price = bundle_impact['sell_price'][idx]\n",
    "    \n",
    "    # find the sell price of the top and low performing articles on that day\n",
    "    day = bundle_impact['calendar_day'][idx]\n",
    "    top_sell_price = data_train_with_dummy_after_bundle[(data_train_with_dummy_after_bundle['article_id'] == top_article_id) & (data_train_with_dummy_after_bundle['calendar_day'] == day)]['sell_price'].values[0]\n",
    "    low_sell_price = data_train_with_dummy_after_bundle[(data_train_with_dummy_after_bundle['article_id'] == other_article_id) & (data_train_with_dummy_after_bundle['calendar_day'] == day)]['sell_price'].values[0]\n",
    "    \n",
    "    # sum the sell price of the top and low performing articles\n",
    "    sum_sell_price = top_sell_price + low_sell_price\n",
    "    \n",
    "    # calculate the price change\n",
    "    price_change = sum_sell_price - bundle_sell_price\n",
    "    \n",
    "    # apply the price change to the original dataset\n",
    "    data_train_with_dummy_after_bundle.loc[(data_train_with_dummy_after_bundle['article_id'] == other_article_id) & (data_train_with_dummy_after_bundle['calendar_day'] == day), 'sell_price'] = low_sell_price - price_change\n",
    "    \n",
    "    # find the rows that have the same calendar day and article id\n",
    "    row = data_train_with_dummy_after_bundle[(data_train_with_dummy_after_bundle['article_id'] == other_article_id) & (data_train_with_dummy_after_bundle['calendar_day'] == day)]\n",
    "    \n",
    "    # use the best model to predict the sales units given the price change\n",
    "    new_sales_units = best_model.predict(row[rf_cols])\n",
    "    \n",
    "    # update the gross profit = sales units * sell price - cost of purchasing + scanback\n",
    "    new_profit = new_sales_units * row['sell_price'] - row['cost_of_purchasing'] + row['scanback']\n",
    "    data_train_with_dummy_after_bundle.loc[(data_train_with_dummy_after_bundle['article_id'] == other_article_id) & (data_train_with_dummy_after_bundle['calendar_day'] == day), 'net_profit'] = new_profit\n",
    "    \n",
    "    # update the sales units\n",
    "    data_train_with_dummy_after_bundle.loc[(data_train_with_dummy_after_bundle['article_id'] == other_article_id) & (data_train_with_dummy_after_bundle['calendar_day'] == day), 'sales_units'] = new_sales_units\n",
    "    \n",
    "# sales after price change\n",
    "sales_after = data_train_with_dummy_after_bundle['sales_units'].sum()\n",
    "sales_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sales Increase: 3091 units\n",
      "Percentage Increase: 0.011582481337722262%\n"
     ]
    }
   ],
   "source": [
    "# calculate the increase in sales after apply 10 bundles\n",
    "sales_increase = round(sales_after - sales_before)\n",
    "print(f\"Sales Increase: {sales_increase} units\")\n",
    "\n",
    "percentage_increase = (sales_increase / sales_before) * 100\n",
    "print(f\"Percentage Increase: {percentage_increase}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Checking Gross Profit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Net Profit Before Bundle Applied: $76394829.54\n",
      "Total Net Profit After Bundle Applied: $76418660.11\n",
      "Net Profit Increase: $23831\n"
     ]
    }
   ],
   "source": [
    "# profit before price change\n",
    "profit_before = data_train_with_dummy['net_profit'].sum()\n",
    "print(\"Total Net Profit Before Bundle Applied: ${:.2f}\".format(profit_before))\n",
    "\n",
    "profit_after = data_train_with_dummy_after_bundle['net_profit'].sum()\n",
    "print(\"Total Net Profit After Bundle Applied: ${:.2f}\".format(profit_after))\n",
    "\n",
    "profit_increase = round(profit_after - profit_before)\n",
    "print(f\"Net Profit Increase: ${profit_increase}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Other Metrics for the Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the regression report on the model\n",
    "def regression_report(model, X_data_unstd, y_data_unstd):\n",
    "    # evaluate the model\n",
    "    y_pred = model.predict(X_data_unstd)\n",
    "    \n",
    "    # rmse, mse, mae\n",
    "    rmse = np.sqrt(mean_squared_error(y_data_unstd, y_pred))\n",
    "    mse = mean_squared_error(y_data_unstd, y_pred)\n",
    "    mae = mean_absolute_error(y_data_unstd, y_pred)\n",
    "    \n",
    "    return rmse, mse, mae\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Mean Squared Error</th>\n",
       "      <th>Mean Absolute Error</th>\n",
       "      <th>Root Mean Squared Error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RF for Promo Sales</td>\n",
       "      <td>3982.585398</td>\n",
       "      <td>21.062069</td>\n",
       "      <td>63.107729</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Model  Mean Squared Error  Mean Absolute Error  \\\n",
       "0  RF for Promo Sales         3982.585398            21.062069   \n",
       "\n",
       "   Root Mean Squared Error  \n",
       "0                63.107729  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the best model and evaluate on the unstandardized whole data\n",
    "best_model = rf_promo\n",
    "\n",
    "y_train, x_train = y_x_split(data_train_with_dummy)\n",
    "y_val, x_val = y_x_split(data_val_with_dummy)\n",
    "y_test, x_test = y_x_split(data_test_with_dummy)\n",
    "\n",
    "# concatenate the training, validation and test data\n",
    "y_data_unstd = pd.concat([y_train, y_val, y_test], ignore_index=True)\n",
    "x_data_unstd = pd.concat([x_train, x_val, x_test], ignore_index=True)\n",
    "\n",
    "# get the fitted features for X\n",
    "x_data_unstd = x_data_unstd[x_train_promo_rf.columns]\n",
    "\n",
    "# get the regression report\n",
    "rmse, mse, mae = regression_report(best_model, x_data_unstd, y_data_unstd)\n",
    "\n",
    "pd.DataFrame({'Model': ['RF for Promo Sales'], \n",
    "                    'Mean Squared Error': [mse], \n",
    "                    'Mean Absolute Error': [mae],\n",
    "                    'Root Mean Squared Error': [rmse]})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
